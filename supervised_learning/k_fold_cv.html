

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>k-Fold Cross-Validation Experiments with Outliers and High Leverage Samples &#8212; Prince&#39;s Data Science Blog</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'supervised_learning/k_fold_cv';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Beta-Variational Autoencoder Experiments" href="../unsupervised_learning/b_vae.html" />
    <link rel="prev" title="Multilayer Perceptron Experiments on Sine Wave Part 2" href="nn_sine_experiments_2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    <p class="title logo__title">Prince's Data Science Blog</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../reinforcement_learning/k_armed_bandit_1.html">K-Armed Bandit Experiments Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reinforcement_learning/k_armed_bandit_2.html">K-Armed Bandit Experiments Part 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reinforcement_learning/db_asynchronous_dp.html">Asynchronous Dynamic Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reinforcement_learning/mc_race_car.html">Monte Carlo Control Experiments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Supervised Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="bias_variance.html">The Bias-Variance Tradeoff</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_sine_experiments_1.html">Multilayer Perceptron Experiments on Sine Wave Part 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_sine_experiments_2.html">Multilayer Perceptron Experiments on Sine Wave Part 2</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">k-Fold Cross-Validation Experiments with Outliers and High Leverage Samples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unsupervised Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unsupervised_learning/b_vae.html">Beta-Variational Autoencoder Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised_learning/kmeans.html">K-Means, Gaussian Mixtures, and the Expectation-Maximization Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised_learning/pca_autoencoder.html">PCA and Neural Network Autoencoder Connection</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Simulations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../simulations/entropy_sim.html">Entropy Simulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../simulations/success_sim.html">Simulating Success</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../causal_inference/ccm_failure_cases.html">Where Convergent Cross-Mapping Fails</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Thoughts/Ideas/Opinions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../thoughts/language_llm.html">On language, LLMs, and why we think certain Egyptian hieroglyphs are about helicopters</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>k-Fold Cross-Validation Experiments with Outliers and High Leverage Samples</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">k-Fold Cross Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#violation-of-the-iid-assumption">Violation of the iid Assumption</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-dataset">Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-setup">Experiment Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-questions">Some Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-details">Experiment Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fold-cv">100-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-if-no-outliers">What if no outliers?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-in-hyperparameter-tuning">Use in Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-outliers">Without Outliers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-outliers">With Outliers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="k-fold-cross-validation-experiments-with-outliers-and-high-leverage-samples">
<h1>k-Fold Cross-Validation Experiments with Outliers and High Leverage Samples<a class="headerlink" href="#k-fold-cross-validation-experiments-with-outliers-and-high-leverage-samples" title="Permalink to this heading">#</a></h1>
<section id="k-fold-cross-validation">
<h2>k-Fold Cross Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this heading">#</a></h2>
<p>When selecting models or model hyperparameters, we wish to ideally select based on which model performs best on all possible values of the data. However, usually, we are only given training data, which do not cover the full support of our data distribution. k-Fold Cross-Validation (k-fold CV) is a technique for estimating the error we expect on data that go beyond the training data (which we’ll call test set) but still come from the same distribution of the training set.</p>
<p>However, because of the way k-fold CV works (which is you shuffle then split into k-groups), it’s possible this becomes problematic for estimating the test performance (or error) when we have rare extreme values of input <span class="math notranslate nohighlight">\(X\)</span> (high leverage datapoints) that result in extreme values of response <span class="math notranslate nohighlight">\(Y\)</span> (outliers). We will use “high-leverage points” and “outliers” interchangeably.</p>
</section>
<section id="violation-of-the-iid-assumption">
<h2>Violation of the iid Assumption<a class="headerlink" href="#violation-of-the-iid-assumption" title="Permalink to this heading">#</a></h2>
<p>k-fold CV splits the training data in a way that violates the iid (independent, indentically-distributed) assumption when we estimate the error, e.g. MSE. Why? Because it splits the data into k-folds without replacement. And putting high leverage/outlier points in one validation fold/group essentially removes the probability it gets selected for the other folds. This could potentially limit its applicability as an estimator of test set error.</p>
</section>
<section id="synthetic-dataset">
<h2>Synthetic Dataset<a class="headerlink" href="#synthetic-dataset" title="Permalink to this heading">#</a></h2>
<p>To test our speculation, we experiment on a synthetic dataset of the form</p>
<p><span class="math notranslate nohighlight">\(Y = \beta_0 + \beta_1 X_1^2 + \beta_2 X_2^2 ... + \beta_p X_p^2 + \epsilon\)</span></p>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is an independent error term taken from a normal distribution <span class="math notranslate nohighlight">\(N(0, 1)\)</span>. Why square the predictors instead of just keep it linear? Making it a simple linear sum will make the problem trivial for a linear regression model, which is the model we use in these experiments. We want to fit a linear model on this quadratic function.</p>
<p>To add high-leverage / outlier points, we randomly select a proportion of samples for each predictor independent of other predictors. Then we multiply those values of <span class="math notranslate nohighlight">\(X_j\)</span> by some multiplier. The responses <span class="math notranslate nohighlight">\(Y_j\)</span> are simply computed using the quadratic model defined above.</p>
<p>We generated 1 million datapoints, 10,000 of these were randomly selected for the train set, while the rest were used for the test set.</p>
</section>
<section id="experiment-setup">
<h2>Experiment Setup<a class="headerlink" href="#experiment-setup" title="Permalink to this heading">#</a></h2>
<p>We first checked the reliability of k-fold CV on this dataset. We chose <span class="math notranslate nohighlight">\(k=100\)</span>. We shuffled the train data, then we split into 100 folds/groups. Then we apply k-fold CV, getting a total of 100 MSEs. We then compute the mean of these MSEs and that becomes the k-fold MSE estimate. But we are not concerned with this particular estimate. We are concerned with the reliability of k-fold CV as a method, so we run 100 trials of 100-fold CV. Each trial we get the mean MSE, each of which is an estimate of the test MSE,</p>
<p><span class="math notranslate nohighlight">\(\hat{\text{MSE}} = \text{mean}({\text{MSE}_{CV}}) \approx \text{MSE}_{test}\)</span>.</p>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this heading">#</a></h2>
<p>We then analyzed the mean and standard deviation of these estimates and found that the standard deviation of the estimate is low and <span class="math notranslate nohighlight">\(\text{mean}(\hat{\text{MSE}}) \pm \text{std}(\hat{\text{MSE}})\)</span> does not cover the ground truth test MSE. The ground truth test MSE is computed by fitting on the entire train set and testing on the test set.</p>
<p>We find that <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> has low variance as an estimator, so running different trials of k-fold CV will more or less yield the same result. So while we will rarely have a case where we will more closely match the <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span>, the consistency of the results is an advantage of k-fold CV in this case.</p>
<p>We then speculated an alternative method that does not suffer the iid violation of k-fold CV. The approach is we simply do a simple validation split, but do it 100 times which we’ll call “fold” for consistency. Each time, we shuffle the data and split into 99% training and 1% validation. This no longer violates the iid assumption because each fold does not rob the other folds of the possibility of picking particular datapoints. We then get the mean of the 100 MSEs. Then again, we are concerned with the reliability of the approach, so we ran 100 trials, with each trial computing the mean of 100 MSEs. We then analyzed the distribution of MSEs and found that the <span class="math notranslate nohighlight">\(\text{mean}(\hat{\text{MSE}}) \pm \text{std}(\hat{\text{MSE}})\)</span> now captures the ground truth test MSE.</p>
<p>However, the new approach has an increased variance such that running any of this 100-“fold” validation to estimate <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> will more likely yield much more varied results.</p>
<p>We tested on the more realistic 10-fold and 5-fold CV and we get similar results.</p>
<p>In the absence of outliers, the two approaches still more or less have the same <span class="math notranslate nohighlight">\(\text{mean}(\hat{\text{MSE}})\)</span> which overestimate the ground truth <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span>. But the standard deviation (relative to the mean) of the second approach is much lower when there are no outliers vs when there are outliers.</p>
<p>Using k-Nearest Neighbors (kNN) regression to investigate the reliability of these two validation approaches to finding the optimal <span class="math notranslate nohighlight">\(k\)</span>, we find that when the data does not contain outliers, the second approach where we do simple random validation T times seems marginally more reliable than k-fold CV. However, we find that in the presence of outliers, simple validation yields high variation in the <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> vs <span class="math notranslate nohighlight">\(k\)</span> charts and yields high variation in the selected optimal <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
<section id="take-aways">
<h2>Take-aways<a class="headerlink" href="#take-aways" title="Permalink to this heading">#</a></h2>
<p>Using linear regression, both k-fold CV and our alternative k-validation method seem to be biased estimates of the <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span> in this particular example. However, k-fold CV has much lower variance than the alternative method with or without outliers. This speaks about its stability as a method.</p>
<p>The second approach is more prone to high variance of predictions especially in the presence of outliers, which may not be desirable. And despite supposedly being more aligned with the iid assumption when estimating a statistic, it seems to be still a biased estimate of <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span>. In our experiments, it also entails high variability in terms of hyperparameter selection, which is not a problem for k-fold CV.</p>
</section>
<section id="some-questions">
<h2>Some Questions<a class="headerlink" href="#some-questions" title="Permalink to this heading">#</a></h2>
<p>We find that it seems k-fold CV has low variance, even if it’s far from the true test MSE. Doing simple validation many times and averaging the results yield similar results as k-fold CV but with higher variance. What’s the explanation for these?</p>
<p>We’re not sure how general these results are or if there are flaws in how we set up these experiments.</p>
</section>
<section id="experiment-details">
<h2>Experiment Details<a class="headerlink" href="#experiment-details" title="Permalink to this heading">#</a></h2>
<p>We define our function as a linear model of <span class="math notranslate nohighlight">\(X_j^2\)</span> predictors with some random noise. In this toy example, we sample the ground truth coefficients from a normal distribution, and we sample <span class="math notranslate nohighlight">\(X\)</span> from a normal distribution as well. A certain proportion of each predictor <span class="math notranslate nohighlight">\(X_j\)</span> is independently chosen and multiplied by a multiplier to serve as high leverage data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># define a ground truth generative function</span>
<span class="k">def</span> <span class="nf">gen_func</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">n_predictors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">p_leverage</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mult_leverage</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_predictors</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n_predictors</span><span class="p">))</span>

    <span class="c1"># for each x_i, we randomly select p_leverage% and multiply by mult_leverage</span>
    <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">p_leverage</span><span class="p">)))</span>
    <span class="n">X</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span> <span class="o">*</span> <span class="n">mult_leverage</span>

    <span class="n">Y</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">X</span><span class="o">**</span><span class="mi">2</span> <span class="o">@</span> <span class="n">coeffs</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># make it squared</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">coeffs</span>
</pre></div>
</div>
</div>
</div>
<p>We generate 1 million datapoints from our generator. 10,000 are randomly selected to be the training data (from which we will derive the validation sets as well)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_train_test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">coeffs</span> <span class="o">=</span> <span class="n">gen_func</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">inds</span><span class="p">)</span>
    <span class="n">inds_train</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[:</span><span class="n">N_train</span><span class="p">]</span>
    <span class="n">inds_test</span> <span class="o">=</span> <span class="n">inds</span><span class="p">[</span><span class="n">N_train</span><span class="p">:]</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inds_train</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">inds_train</span><span class="p">]</span>

    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">inds_test</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">inds_test</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:                &quot;</span><span class="p">,</span> <span class="n">coeffs</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;len(X_train), len(X_test):   &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;len (Y_train), len(Y_test):  &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">n_predictors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">p_leverage</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">mult_leverage</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">gen_train_test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficients:                 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
len(X_train), len(X_test):    10000 990000
len (Y_train), len(Y_test):   10000 990000
</pre></div>
</div>
</div>
</div>
<p>We visualize below each <span class="math notranslate nohighlight">\(X_j\)</span> vs <span class="math notranslate nohighlight">\(Y\)</span> in our training data</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;X_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Y&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;X vs Y&quot;</span><span class="p">)</span>

        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;X_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">loglog</span><span class="p">()</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;log(Y)&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;log(X)&quot;</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;log(X) vs log(Y)&quot;</span><span class="p">)</span>    
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>    
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/5363e7597d867229f67b829697475668d2d0d92cd1a0eef2d03ba016e959b7ea.png" src="../_images/5363e7597d867229f67b829697475668d2d0d92cd1a0eef2d03ba016e959b7ea.png" />
</div>
</div>
<section id="fold-cv">
<h3>100-fold CV<a class="headerlink" href="#fold-cv" title="Permalink to this heading">#</a></h3>
<p>We run k-fold CV over many trials using linear regression. Each k-fold CV trial yields an estimate <span class="math notranslate nohighlight">\(\hat{\text{MSE}} = \text{mean}({\text{MSE}_{CV}}) \approx \text{MSE}_{test}\)</span>. Running many trials, we can get a mean and standard deviation of the <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> estimates, which we compare with the ground truth <span class="math notranslate nohighlight">\(MSE_{test}\)</span></p>
<p>The chart below shows the highly skewed <span class="math notranslate nohighlight">\({\text{MSE}_{CV}}\)</span> values for one trial of k-fold CV. This is due to the presence of high-leverage/outlier points in the training data. But we are not concerned with the distribution of errors in one trial of k-fold CV. We are concerned with the distribution of the estimates <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>

<span class="k">def</span> <span class="nf">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>

    <span class="n">k_folds_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)),</span> <span class="n">k_folds</span><span class="p">))</span>
    <span class="n">mse_means</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">mse_stds</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>

        <span class="n">inds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">inds</span><span class="p">)</span>
        <span class="n">X_train_shuffled</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span>
        <span class="n">Y_train_shuffled</span> <span class="o">=</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">inds</span><span class="p">]</span>

        <span class="n">mse_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_folds</span><span class="p">):</span>

            <span class="n">k_folds_idx_retain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k_folds</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">sample_idx_train</span> <span class="o">=</span> <span class="n">k_folds_idx</span><span class="p">[</span><span class="n">k_folds_idx_retain</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">sample_idx_valid</span> <span class="o">=</span> <span class="n">k_folds_idx</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

            <span class="n">X_fold_train</span> <span class="o">=</span> <span class="n">X_train_shuffled</span><span class="p">[</span><span class="n">sample_idx_train</span><span class="p">]</span>
            <span class="n">X_fold_valid</span> <span class="o">=</span> <span class="n">X_train_shuffled</span><span class="p">[</span><span class="n">sample_idx_valid</span><span class="p">]</span>

            <span class="n">Y_fold_train</span> <span class="o">=</span> <span class="n">Y_train_shuffled</span><span class="p">[</span><span class="n">sample_idx_train</span><span class="p">]</span>
            <span class="n">Y_fold_valid</span> <span class="o">=</span> <span class="n">Y_train_shuffled</span><span class="p">[</span><span class="n">sample_idx_valid</span><span class="p">]</span>

            <span class="c1"># Create linear regression object</span>
            <span class="n">regr</span> <span class="o">=</span> <span class="n">func</span>

            <span class="c1"># Train the model using the training sets</span>
            <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_fold_train</span><span class="p">,</span> <span class="n">Y_fold_train</span><span class="p">)</span>

            <span class="c1"># Make predictions using the testing set</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_fold_valid</span><span class="p">)</span>

            <span class="n">mse_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_fold_valid</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)]</span>       

        <span class="n">mse_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>    

        <span class="c1"># compute the mean MSE from cross validation</span>
        <span class="n">mse_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>

        <span class="c1"># compute the standard deviation</span>
        <span class="n">mse_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>    

        <span class="n">mse_means</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mse_mean</span><span class="p">]</span>
        <span class="n">mse_stds</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mse_std</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">trial</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">plot</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>        
            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mse_list</span><span class="p">,</span> <span class="n">cumulative</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_list</span><span class="p">,</span> <span class="n">showfliers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSE histogram for 1 CV trial&quot;</span><span class="p">)</span>        
            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSEs with outliers for 1 CV trial&quot;</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSEs without outliers for 1 CV trial&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>      
    
    <span class="k">return</span> <span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We define 100 k-folds and 100 trials (100-fold each).</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># do standard k-fold cross validation</span>
<span class="n">k_folds</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/422bb8e96ecd541ee812450474ae8069c023114c520d63cf06aa907c956a6253.png" src="../_images/422bb8e96ecd541ee812450474ae8069c023114c520d63cf06aa907c956a6253.png" />
</div>
</div>
<p>We show below the distribution of <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> from 100 trials</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>        
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">cumulative</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">showfliers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSE Means histogram for </span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="s2"> CV trials&quot;</span><span class="p">)</span>        
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSEs with outliers for </span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="s2"> CV trials&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;MSEs without outliers for </span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="s2"> CV trials&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>    

<span class="n">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>       
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/9903cef409112f640668aaffebbe4cf4ee1a609788d4fe7c521689278515afa4.png" src="../_images/9903cef409112f640668aaffebbe4cf4ee1a609788d4fe7c521689278515afa4.png" />
</div>
</div>
<p>We computed the ground truth <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span> as well as the mean and standard deviation of <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span>. We find that <span class="math notranslate nohighlight">\(\text{mean}(\hat{\text{MSE}}) \pm \text{std}(\hat{\text{MSE}})\)</span> does not cover <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span>. However, the estimated <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> has low variance so it speaks about the consistency of the results when using k-fold CV for this problem.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="c1"># compute the mean MSE from cross validation</span>
    <span class="n">mse_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>

    <span class="c1"># compute the standard deviation</span>
    <span class="n">mse_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>

    <span class="c1"># compute the true MSE from the test set when trained on the entire train set</span>
    <span class="c1"># Create linear regression object</span>
    <span class="n">regr</span> <span class="o">=</span> <span class="n">func</span>

    <span class="c1"># Train the model using the training sets</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

    <span class="c1"># Make predictions using the testing set</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">mse_gt</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean MSE         &quot;</span><span class="p">,</span> <span class="n">mse_mean</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std Dev MSE      &quot;</span><span class="p">,</span> <span class="n">mse_std</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ground truth MSE &quot;</span><span class="p">,</span> <span class="n">mse_gt</span><span class="p">)</span>


<span class="n">func</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          63.75660471426123
Std Dev MSE       0.15955756270713287
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
<p>The problem with k-fold CV is if you have rare outliers/high leverage points, it’s possible they get concentrated in one validation fold/group. And this causes the distribution of errors to be highly skewed. Once these points are chosen in one fold, they’re no longer available for the other folds. Thus each k-fold becomes dependent on others, and no longer iid. This violates the assumption of the Central Limit Theorem and prevents us from computing say the standard error properly (which requires iid samples).</p>
<p>How might we go around this? What if instead of regular k-fold, we run a simple train-valid set split but run it 100 times, randomizing each time. With this, we make each validation MSE an iid sample because even if we got datapoints <span class="math notranslate nohighlight">\(X_j, Y_j\)</span> the other “folds” of train-valid splits will still have the chance to select them.</p>
<p>Again, we show below the distribution of <span class="math notranslate nohighlight">\(\text{MSE}_{CV}\)</span> for a single trial (100 validation runs). But we are concerned with the reliability of this estimate, so we run 100 trials.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_kfold2_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
    <span class="n">mse_means</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">mse_stds</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">trial</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>

        <span class="n">mse_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_folds</span><span class="p">):</span>

            <span class="n">sample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">sample_idx</span><span class="p">)</span>
            <span class="n">train_idx</span> <span class="o">=</span> <span class="n">sample_idx</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">//</span><span class="n">k_folds</span><span class="p">:]</span>
            <span class="n">valid_idx</span> <span class="o">=</span> <span class="n">sample_idx</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">//</span><span class="n">k_folds</span><span class="p">]</span>

            <span class="n">X_fold_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
            <span class="n">X_fold_valid</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">valid_idx</span><span class="p">]</span>

            <span class="n">Y_fold_train</span> <span class="o">=</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
            <span class="n">Y_fold_valid</span> <span class="o">=</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">valid_idx</span><span class="p">]</span>

            <span class="c1"># Create linear regression object</span>
            <span class="n">regr</span> <span class="o">=</span> <span class="n">func</span>

            <span class="c1"># Train the model using the training sets</span>
            <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_fold_train</span><span class="p">,</span> <span class="n">Y_fold_train</span><span class="p">)</span>

            <span class="c1"># Make predictions using the testing set</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_fold_valid</span><span class="p">)</span>
            <span class="n">mse_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_fold_valid</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)]</span>       

        <span class="n">mse_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>    

        <span class="c1"># compute the mean MSE from cross validation</span>
        <span class="n">mse_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>

        <span class="c1"># compute the standard deviation</span>
        <span class="n">mse_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>    

        <span class="n">mse_means</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mse_mean</span><span class="p">]</span>
        <span class="n">mse_stds</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mse_std</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">trial</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">plot</span><span class="p">:</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>        
            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">mse_list</span><span class="p">,</span> <span class="n">cumulative</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_list</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">mse_list</span><span class="p">,</span> <span class="n">showfliers</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSE histogram for 1 CV trial&quot;</span><span class="p">)</span>        
            <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSEs with outliers for 1 CV trial&quot;</span><span class="p">)</span>
            <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSEs without outliers for 1 CV trial&quot;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  

    <span class="k">return</span> <span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span>     
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_folds</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold2_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/f3c37e85b00829e2fd6345f1cf59ab991319d8b16009b7393b76a533d5522070.png" src="../_images/f3c37e85b00829e2fd6345f1cf59ab991319d8b16009b7393b76a533d5522070.png" />
</div>
</div>
<p>We show below the distribution of <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> from 100 trials</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/c9283d76da045f79ae03c7176b4308d44c7c72d2eacb2688df2a58ea75ac72f4.png" src="../_images/c9283d76da045f79ae03c7176b4308d44c7c72d2eacb2688df2a58ea75ac72f4.png" />
</div>
</div>
<p>We find that <span class="math notranslate nohighlight">\(\text{mean}(\hat{\text{MSE}}) \pm \text{std}(\hat{\text{MSE}})\)</span> covers <span class="math notranslate nohighlight">\(\text{MSE}_{test}\)</span>. But the increased variance means we expect different results each run of 100-“fold” validation.</p>
<p>Is this approach more reliable than conventional k-fold? How general is this result?</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          37.47142301010411
Std Dev MSE       1.4210854715202004e-14
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>10-fold CV<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The problem with our experiments above is that our training set is small yet we’re using 100-fold validation. This will make the validation runs correlated since chances are they will share much of the dataset with each other. So now, we experiment on the more realistic 10-fold validation. We still run over 100 trials.</p>
<p>We show below the results of a sample MSE histogram for one trial of 10-fold CV (top charts) and we show the distribution of values of <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> over 100 trials of 10-fold CV. We find that we get more or less the same result as the 100-fold CV.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/21c52b0811b52cec25129f3666ad3304c4854e8f629e2d882a3a2db55616d4ff.png" src="../_images/21c52b0811b52cec25129f3666ad3304c4854e8f629e2d882a3a2db55616d4ff.png" />
<img alt="../_images/623ccf0c394c8772fd7def3d9a42f6bf0071c8e657b2bd8108b6457b65a5f9a4.png" src="../_images/623ccf0c394c8772fd7def3d9a42f6bf0071c8e657b2bd8108b6457b65a5f9a4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          63.974719087827445
Std Dev MSE       0.6883531568069371
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
<p>We show the results for the alternative approach. We also get more or less the same results, with some increase in the estimated <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span></p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/1f4061073015d27643006d06885c10b79d699a03f4728f389881782b71686d96.png" src="../_images/1f4061073015d27643006d06885c10b79d699a03f4728f389881782b71686d96.png" />
<img alt="../_images/12108629eb28931f9c2059796db1886d298c3efd68c873cbe27d24e148d5e31d.png" src="../_images/12108629eb28931f9c2059796db1886d298c3efd68c873cbe27d24e148d5e31d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          48.73814281743391
Std Dev MSE       7.105427357601002e-15
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3>5-fold CV<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>There’s no marked changes when we use 5-fold CV. We show below the results of k-fold CV.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/e942923db301150737569bb08085e0ba083de599a14300da76d2285e27a7a41d.png" src="../_images/e942923db301150737569bb08085e0ba083de599a14300da76d2285e27a7a41d.png" />
<img alt="../_images/1418ff2b356fbfd59861a2f8e63c848714d09ece70bfdfeb7601b5b5b00b7867.png" src="../_images/1418ff2b356fbfd59861a2f8e63c848714d09ece70bfdfeb7601b5b5b00b7867.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          64.47303996932304
Std Dev MSE       1.1310935713605763
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
<p>We show below the results of the alternative approach.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d078f2b85d273054c27e261dff0dd812f5f22bf7205c962243d24ce057821d3c.png" src="../_images/d078f2b85d273054c27e261dff0dd812f5f22bf7205c962243d24ce057821d3c.png" />
<img alt="../_images/201c3423316d3006cb91f385da4fa61e0bc38fc332cf552bdb166b2f210b7b98.png" src="../_images/201c3423316d3006cb91f385da4fa61e0bc38fc332cf552bdb166b2f210b7b98.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          28.81663603726913
Std Dev MSE       0.0
Ground truth MSE  53.04536320061062
</pre></div>
</div>
</div>
</div>
</section>
<section id="what-if-no-outliers">
<h3>What if no outliers?<a class="headerlink" href="#what-if-no-outliers" title="Permalink to this heading">#</a></h3>
<p>We generate data with no outliers. We find that in both validaation approaches, the test MSE is over-estimated. Meanwhile the second approach’s standard deviation, relative to its mean, is much lower than when there are outliers. It seems the second approach is more sensitive to the presence of outliers.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">n_predictors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">p_leverage</span><span class="o">=</span><span class="mf">0.000</span><span class="p">,</span> <span class="n">mult_leverage</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">gen_train_test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficients:                 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
len(X_train), len(X_test):    10000 990000
len (Y_train), len(Y_test):   10000 990000
</pre></div>
</div>
<img alt="../_images/a1aafcf150af48100695e64ea691353e935aeb07297702c9f364a8d687ea290f.png" src="../_images/a1aafcf150af48100695e64ea691353e935aeb07297702c9f364a8d687ea290f.png" />
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/d7c7a1d0849f03e209635ca7609c7c3d04e42e282a5d6aae189971f3e28ef8b8.png" src="../_images/d7c7a1d0849f03e209635ca7609c7c3d04e42e282a5d6aae189971f3e28ef8b8.png" />
<img alt="../_images/5ec20fa15834fb3f62ca19982db8379114d18c75f4fbbf3e27647931cf7a8fac.png" src="../_images/5ec20fa15834fb3f62ca19982db8379114d18c75f4fbbf3e27647931cf7a8fac.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          6.420870371804342
Std Dev MSE       0.00300694628307649
Ground truth MSE  6.108880543132803
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/af32303206e6e9abfa85eb390ad4e2f59581ef2a75d568c2b416596a3d5163fe.png" src="../_images/af32303206e6e9abfa85eb390ad4e2f59581ef2a75d568c2b416596a3d5163fe.png" />
<img alt="../_images/da6468325113b201c6cf4c618817fd18d0ac21bfcb95838a3adeb1dac9e477d7.png" src="../_images/da6468325113b201c6cf4c618817fd18d0ac21bfcb95838a3adeb1dac9e477d7.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          6.513932692733079
Std Dev MSE       8.881784197001252e-16
Ground truth MSE  6.108880543132803
</pre></div>
</div>
</div>
</div>
</section>
<section id="use-in-hyperparameter-tuning">
<h3>Use in Hyperparameter Tuning<a class="headerlink" href="#use-in-hyperparameter-tuning" title="Permalink to this heading">#</a></h3>
<section id="without-outliers">
<h4>Without Outliers<a class="headerlink" href="#without-outliers" title="Permalink to this heading">#</a></h4>
<p>Because linear regression has no hyperparameters, we experiment using kNN regressor instead. We first analyze the dataset without outliers.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">n_predictors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">p_leverage</span><span class="o">=</span><span class="mf">0.000</span><span class="p">,</span> <span class="n">mult_leverage</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">gen_train_test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficients:                 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
len(X_train), len(X_test):    10000 990000
len (Y_train), len(Y_test):   10000 990000
</pre></div>
</div>
<img alt="../_images/a1aafcf150af48100695e64ea691353e935aeb07297702c9f364a8d687ea290f.png" src="../_images/a1aafcf150af48100695e64ea691353e935aeb07297702c9f364a8d687ea290f.png" />
</div>
</div>
<p>It looks like in both validation approaches, the estimated MSE overestimates the ground truth test MSE. However, in terms of selecting the optimal parameter <span class="math notranslate nohighlight">\(k\)</span>, the second approach where we do simple validation T times, has more variance in its MSE vs k curves. But it has higher chances of picking the correct <span class="math notranslate nohighlight">\(k\)</span> compared to k-fold CV.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">import</span> <span class="nn">statistics</span> <span class="k">as</span> <span class="nn">st</span>

<span class="k">def</span> <span class="nf">hyperparam_run</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> 
                   <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kfold&#39;</span><span class="p">,</span> <span class="c1"># kfold or kfold2</span>
                   <span class="n">trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>    
    <span class="n">opt_ks</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># optimal ks</span>
    <span class="n">mse_means_test</span> <span class="o">=</span> <span class="p">[]</span>    
    <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trials</span><span class="p">):</span>
        <span class="n">mse_means_valid</span> <span class="o">=</span> <span class="p">[]</span>    
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_vals</span><span class="p">:</span>
            <span class="c1"># do standard k-fold cross validation</span>
            <span class="n">func</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        
            <span class="n">trials</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kfold&#39;</span><span class="p">:</span>
                <span class="n">mse_means_</span><span class="p">,</span> <span class="n">mse_stds_</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;kfold2&#39;</span><span class="p">:</span>
                <span class="n">mse_means_</span><span class="p">,</span> <span class="n">mse_stds_</span> <span class="o">=</span> <span class="n">run_kfold2_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
            <span class="n">mse_means_valid</span> <span class="o">+=</span> <span class="n">mse_means_</span>

            <span class="c1"># test mse</span>
            <span class="c1"># train on all</span>
            <span class="k">if</span> <span class="n">seed</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># just once</span>
                <span class="n">func</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
                <span class="c1"># Make predictions using the testing set</span>
                <span class="n">y_pred</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
                <span class="n">mse_gt</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
                <span class="n">mse_means_test</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mse_gt</span><span class="p">]</span>

        <span class="n">k_min_mse_valid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mse_means_valid</span><span class="p">)</span>
        <span class="n">opt_ks</span> <span class="o">+=</span> <span class="p">[</span><span class="n">k_min_mse_valid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">mse_means_valid</span><span class="p">)</span> <span class="c1">#label=&#39;validation&#39;)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">k_min_mse_valid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mse_means_valid</span><span class="p">[</span><span class="n">k_min_mse_valid</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="n">k_min_mse_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mse_means_test</span><span class="p">)</span>
    <span class="n">opt_k_test</span> <span class="o">=</span> <span class="n">k_min_mse_test</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">opt_k</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">opt_ks</span><span class="p">)</span> <span class="c1"># get the most frequent one, which is the likeliest the method will return        </span>
    
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Count Neighbors (k)&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Mean of MSE&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation MSE </span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="s2"> Trials | Most likely optimal k = </span><span class="si">{</span><span class="n">opt_k</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">mse_means_test</span><span class="p">)</span> <span class="c1">#label=&#39;test&#39;)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">opt_k_test</span><span class="p">,</span> <span class="n">mse_means_test</span><span class="p">[</span><span class="n">k_min_mse_test</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Count Neighbors (k)&quot;</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">)</span> 
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test MSE | Optimal k = </span><span class="si">{</span><span class="n">opt_k_test</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>       
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">opt_k</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We show the results using 5-fold CV. For the MSE vs k curves, we run 10 trials and indicate the minimum MSE with a red X in each trial (top charts). The middle charts show the distribution MSE per fold from a single trial of validation run, while the bottom charts show the distribution of the estimated <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> over 100 trials.</p>
<p>We find that <span class="math notranslate nohighlight">\(k=3\)</span> is the likeliest hyperparameter to be chosen, while the ground truth value is <span class="math notranslate nohighlight">\(k=4\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_folds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">opt_k</span> <span class="o">=</span> <span class="n">hyperparam_run</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kfold&#39;</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_folds</span><span class="o">=</span><span class="n">k_folds</span><span class="p">)</span>

<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">opt_k</span><span class="p">)</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
<span class="n">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>
<span class="n">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/8df55f9554b27757bc41c23cf0babdcc70ddb86121f91fc8640cb5ea239761d3.png" src="../_images/8df55f9554b27757bc41c23cf0babdcc70ddb86121f91fc8640cb5ea239761d3.png" />
<img alt="../_images/c25a9af33c4233b443fc448ef63c69eefaffc4dfb1a00311a0320e38af275012.png" src="../_images/c25a9af33c4233b443fc448ef63c69eefaffc4dfb1a00311a0320e38af275012.png" />
<img alt="../_images/2b775a362de785b7d1ee762449c96ee5cad89bcbd7a4eef39283af86309a6cd3.png" src="../_images/2b775a362de785b7d1ee762449c96ee5cad89bcbd7a4eef39283af86309a6cd3.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          0.7305736586480543
Std Dev MSE       0.012843940843275855
Ground truth MSE  0.6389746721462332
</pre></div>
</div>
</div>
</div>
<p>We show the results using the second approach with 5 “fold” validation runs. For the MSE vs k curves, we run 10 trials of this validation approach, and indicate the minimum MSE with a red X in each trial (top charts). The middle charts show the distribution MSE per fold from a single trial of validation run, while the bottom charts show the distribution of the estimated <span class="math notranslate nohighlight">\(\hat{\text{MSE}}\)</span> over 100 trials.</p>
<p>We find that <span class="math notranslate nohighlight">\(k=4\)</span> is the likeliest hyperparameter to be chosen, which is the same as the ground truth value. However, there are still chances to pick <span class="math notranslate nohighlight">\(k=3\)</span></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_folds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">opt_k</span> <span class="o">=</span> <span class="n">hyperparam_run</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kfold2&#39;</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_folds</span><span class="o">=</span><span class="n">k_folds</span><span class="p">)</span>

<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">opt_k</span><span class="p">)</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
<span class="n">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>
<span class="n">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/813668413b4f49db316145d7a24bcca805d15a0eaafca87b46e98145c688e9e2.png" src="../_images/813668413b4f49db316145d7a24bcca805d15a0eaafca87b46e98145c688e9e2.png" />
<img alt="../_images/33873ba270703c0df898ba7fe70c4e58365cd191516db0c522a56ab9cd7ee2fb.png" src="../_images/33873ba270703c0df898ba7fe70c4e58365cd191516db0c522a56ab9cd7ee2fb.png" />
<img alt="../_images/f20a7b00bbafbf41d8d92a15a81be87d784439236d0e021b52f107f7da287b06.png" src="../_images/f20a7b00bbafbf41d8d92a15a81be87d784439236d0e021b52f107f7da287b06.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          0.7382083355193587
Std Dev MSE       0.012025683367967064
Ground truth MSE  0.6343301684055197
</pre></div>
</div>
</div>
</div>
</section>
<section id="with-outliers">
<h4>With Outliers<a class="headerlink" href="#with-outliers" title="Permalink to this heading">#</a></h4>
<p>Next we analyze the dataset with outliers.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">N_train</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">kwargs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">n_predictors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">p_leverage</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">mult_leverage</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">gen_train_test</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficients:                 [ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337]
len(X_train), len(X_test):    10000 990000
len (Y_train), len(Y_test):   10000 990000
</pre></div>
</div>
<img alt="../_images/5363e7597d867229f67b829697475668d2d0d92cd1a0eef2d03ba016e959b7ea.png" src="../_images/5363e7597d867229f67b829697475668d2d0d92cd1a0eef2d03ba016e959b7ea.png" />
</div>
</div>
<p>In the presence of outliers, the second approach of T trials of simple validation yields highly variable MSE vs <span class="math notranslate nohighlight">\(k\)</span> curves and can sometimes yield <span class="math notranslate nohighlight">\(k\)</span> values that are widely different from the ground truth optimal value (in this case <span class="math notranslate nohighlight">\(k=1\)</span>).</p>
<p>Meanwhile, k-fold CV yields a more stable set of MSE vs <span class="math notranslate nohighlight">\(k\)</span> curves and mostly returned <span class="math notranslate nohighlight">\(k=2\)</span> and rarely returned <span class="math notranslate nohighlight">\(k=1\)</span>. It seems in the presence of outliers, k-fold CV is more reliable.</p>
<p>We show the results using 5-fold CV</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_folds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">opt_k</span> <span class="o">=</span> <span class="n">hyperparam_run</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kfold&#39;</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_folds</span><span class="o">=</span><span class="n">k_folds</span><span class="p">)</span>

<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">opt_k</span><span class="p">)</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
<span class="n">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>
<span class="n">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/525bd761c15fb32c57c9b50b2c9932e00355a33cd0e5e950dd1b986e52204531.png" src="../_images/525bd761c15fb32c57c9b50b2c9932e00355a33cd0e5e950dd1b986e52204531.png" />
<img alt="../_images/571a3907b10c12f79e1ac5ee3782b5efd7df61e6730f4750ec0c61bee41662de.png" src="../_images/571a3907b10c12f79e1ac5ee3782b5efd7df61e6730f4750ec0c61bee41662de.png" />
<img alt="../_images/15eb2a0132906c8bfc54a0e2eb69d48c1864fd9b66f65b5d049afffb7b02ad13.png" src="../_images/15eb2a0132906c8bfc54a0e2eb69d48c1864fd9b66f65b5d049afffb7b02ad13.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          51.95621758878725
Std Dev MSE       0.26409296852545755
Ground truth MSE  33.439163683759844
</pre></div>
</div>
</div>
</div>
<p>We show the results using the second approach with 5 “fold” validation runs. We note the highly variable MSE vs <span class="math notranslate nohighlight">\(k\)</span> curves.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_folds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">opt_k</span> <span class="o">=</span> <span class="n">hyperparam_run</span><span class="p">(</span><span class="n">k_vals</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;kfold2&#39;</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">k_folds</span><span class="o">=</span><span class="n">k_folds</span><span class="p">)</span>

<span class="n">trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">opt_k</span><span class="p">)</span>
<span class="n">mse_means</span><span class="p">,</span> <span class="n">mse_stds</span> <span class="o">=</span> <span class="n">run_kfold_trials</span><span class="p">(</span><span class="n">k_folds</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
<span class="n">plot_mse_dists</span><span class="p">(</span><span class="n">mse_means</span><span class="p">)</span>
<span class="n">compare_w_test_mse</span><span class="p">(</span><span class="n">mse_means</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/e2e40224999f6fab3656727ee6e35c00a0446251a59a0d5a89c933d4bbc0d31b.png" src="../_images/e2e40224999f6fab3656727ee6e35c00a0446251a59a0d5a89c933d4bbc0d31b.png" />
<img alt="../_images/571a3907b10c12f79e1ac5ee3782b5efd7df61e6730f4750ec0c61bee41662de.png" src="../_images/571a3907b10c12f79e1ac5ee3782b5efd7df61e6730f4750ec0c61bee41662de.png" />
<img alt="../_images/15eb2a0132906c8bfc54a0e2eb69d48c1864fd9b66f65b5d049afffb7b02ad13.png" src="../_images/15eb2a0132906c8bfc54a0e2eb69d48c1864fd9b66f65b5d049afffb7b02ad13.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean MSE          51.95621758878725
Std Dev MSE       0.26409296852545755
Ground truth MSE  33.439163683759844
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./supervised_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="nn_sine_experiments_2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Multilayer Perceptron Experiments on Sine Wave Part 2</p>
      </div>
    </a>
    <a class="right-next"
       href="../unsupervised_learning/b_vae.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Beta-Variational Autoencoder Experiments</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-fold-cross-validation">k-Fold Cross Validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#violation-of-the-iid-assumption">Violation of the iid Assumption</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#synthetic-dataset">Synthetic Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-setup">Experiment Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#take-aways">Take-aways</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-questions">Some Questions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiment-details">Experiment Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fold-cv">100-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">10-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5-fold CV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-if-no-outliers">What if no outliers?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-in-hyperparameter-tuning">Use in Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#without-outliers">Without Outliers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#with-outliers">With Outliers</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Prince Javier
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>